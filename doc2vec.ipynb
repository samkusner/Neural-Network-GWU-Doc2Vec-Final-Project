{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Current imports\n",
    "import pandas as pd\n",
    "\n",
    "##Tensorflow\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Embedding, GlobalAveragePooling1D, Dense, Input, Flatten, Dropout\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment\n",
       "0  One of the other reviewers has mentioned that ...  positive\n",
       "1  A wonderful little production. <br /><br />The...  positive\n",
       "2  I thought this was a wonderful way to spend ti...  positive\n",
       "3  Basically there's a family where a little boy ...  negative\n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...  positive"
      ]
     },
     "execution_count": 287,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##Read in CSV\n",
    "df = pd.read_csv('IMDB_Dataset.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    1\n",
      "1    1\n",
      "2    1\n",
      "3    0\n",
      "4    1\n",
      "Name: sentiment, dtype: int64\n",
      "0    One of the other reviewers has mentioned that ...\n",
      "1    A wonderful little production. <br /><br />The...\n",
      "2    I thought this was a wonderful way to spend ti...\n",
      "3    Basically there's a family where a little boy ...\n",
      "4    Petter Mattei's \"Love in the Time of Money\" is...\n",
      "Name: review, dtype: object\n"
     ]
    }
   ],
   "source": [
    "##do this to map each categorical variable as a 1 or 0\n",
    "y=df['sentiment'].map({'positive': 1, 'negative': 0})\n",
    "print(y.head())\n",
    "\n",
    "reviews=df['review']\n",
    "print(reviews.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Preprocessing libraries\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Preprocessing code\n",
    "##remove stop words?\n",
    "##lowercase or standardize the format or each word? \n",
    "##tokenizer calls .lower()\n",
    "\n",
    "##standard tokenizing code\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(reviews)\n",
    "sequences = tokenizer.texts_to_sequences(reviews)\n",
    "word_index = tokenizer.word_index\n",
    "padded_sequences = pad_sequences(sequences)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[    0     0     0 ...   125  4103   486]\n",
      " [    0     0     0 ...  1977    69   221]\n",
      " [    0     0     0 ...    63    16   350]\n",
      " ...\n",
      " [    0     0     0 ... 22840     2  6050]\n",
      " [    0     0     0 ...    67   739    42]\n",
      " [    0     0     0 ...   794    11    17]]\n",
      "(50000, 2493)\n"
     ]
    }
   ],
   "source": [
    "print(padded_sequences)\n",
    "print(padded_sequences.shape)\n",
    "# 50,000 sequences padded to a length of 2493"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After experimentation, cross validation is important!!!\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(padded_sequences, y, test_size=0.4, random_state=22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment and change this\n",
    "EMBEDDING_DIM = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KerasTensor(type_spec=TensorSpec(shape=(None, 2493), dtype=tf.float32, name='input_46'), name='input_46', description=\"created by layer 'input_46'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 2493), dtype=tf.float32, name='input_47'), name='input_47', description=\"created by layer 'input_47'\")\n"
     ]
    }
   ],
   "source": [
    "# Testing environment\n",
    "print(Input(shape=(padded_sequences.shape[1],)))\n",
    "print(Input(shape=(X_train.shape[1],)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [],
   "source": [
    "##initial doc2vec model only on IMDB_Dataset.csv\n",
    "def doc2vec_model():\n",
    "    \n",
    "    ##We have a 2D representation for the tensor, reduce to 1D which is what we need\n",
    "    ##first dimension will have variable size and second dimension is 2493\n",
    "\n",
    "    input_layer = Input(shape=(X_train.shape[1],))\n",
    "\n",
    "\n",
    "    #Embedding layer\n",
    "    ##word_index is a dictionary for our vocabulary, add +1 for the padding token\n",
    "    embedding_layer = Embedding(input_dim=len(word_index) + 1, output_dim=EMBEDDING_DIM)(input_layer)\n",
    "\n",
    "    #GlobalAveragePooling1D layer\n",
    "    ##average_pooling = GlobalAveragePooling1D()(embedding_layer)\n",
    "\n",
    "    #Flattening\n",
    "    flatten = Flatten()(embedding_layer)\n",
    "\n",
    "    #Dense layers and dropout to help with overfitting\n",
    "    dense_1 = Dense(64, activation='relu')(flatten)\n",
    "    dropout_1 = Dropout(0.2)(dense_1)\n",
    "\n",
    "    dense_2 = Dense(32, activation='relu')(dropout_1)\n",
    "    dropout_2 = Dropout(0.1)(dense_2)\n",
    "\n",
    "\n",
    "    output_layer = Dense(1, activation='sigmoid')(dropout_2)\n",
    "\n",
    "\n",
    "    model = Model(inputs=input_layer, outputs=output_layer)\n",
    "\n",
    "    #binary or multi-class compilation\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy', 'mae'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "3000/3000 [==============================] - 354s 118ms/step - loss: 0.3837 - accuracy: 0.8134 - mae: 0.2430\n",
      "Epoch 2/2\n",
      "3000/3000 [==============================] - 368s 123ms/step - loss: 0.1224 - accuracy: 0.9567 - mae: 0.0674\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x2b930e0d1b0>"
      ]
     },
     "execution_count": 296,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##Train actual model\n",
    "model = doc2vec_model()\n",
    "model.fit(X_train, y_train, batch_size=10, epochs=2, verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000/2000 [==============================] - 14s 7ms/step - loss: 0.3093 - accuracy: 0.8874 - mae: 0.1345\n",
      "Test Loss: 0.30932852625846863\n",
      "Test Accuracy: 0.8873500227928162\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the test set\n",
    "results = model.evaluate(X_test, y_test, batch_size=10)\n",
    "\n",
    "# Print the evaluation results\n",
    "print(\"Test Loss:\", results[0])\n",
    "print(\"Test Accuracy:\", results[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Use new dataset of movie reviews and see how it performs\n",
    "import os\n",
    "\n",
    "def read_txt_files(path):\n",
    "    file_contents = []\n",
    "    # Iterate over each file in the directory\n",
    "    for file in os.listdir(path):\n",
    "        if file.endswith(\".txt\"):\n",
    "            file_path = os.path.join(path, file)\n",
    "            \n",
    "            # Read the content of the file and append it to the list\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                content = f.read()\n",
    "                file_contents.append(content)\n",
    "\n",
    "    return file_contents\n",
    "\n",
    "##iterate and get reviews from each .txt file\n",
    "pos_list = read_txt_files('pos')\n",
    "neg_list = read_txt_files('neg')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [],
   "source": [
    "##line up 1's and 0's with txt file reviews\n",
    "pos_list_bin = [1 for i in list(range(12500))]\n",
    "neg_list_bin = [0 for i in list(range(12500))]\n",
    "\n",
    "##line up labels with txt file sentiment\n",
    "list_of_strings = pos_list + neg_list\n",
    "bin_list = pos_list_bin + neg_list_bin\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [],
   "source": [
    "##tokenizer to process data into same format that model had\n",
    "tokenizer_test = Tokenizer()\n",
    "tokenizer_test.fit_on_texts(list_of_strings)\n",
    "sequences_test = tokenizer_test.texts_to_sequences(list_of_strings)\n",
    "word_index_test = tokenizer_test.word_index\n",
    "padded_test = pad_sequences(sequences_test, maxlen=2493)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert this to a pd series\n",
    "bin_list = pd.Series(bin_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "<class 'pandas.core.series.Series'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'pandas.core.series.Series'>\n"
     ]
    }
   ],
   "source": [
    "##Testing environment\n",
    "print(type(padded_sequences))\n",
    "print(type(y))\n",
    "print(type(padded_test))\n",
    "print(type(bin_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2500/2500 [==============================] - 17s 7ms/step - loss: 1.2814 - accuracy: 0.5649 - mae: 0.4432\n",
      "Test Loss: 1.2814308404922485\n",
      "Test Accuracy: 0.5649200081825256\n"
     ]
    }
   ],
   "source": [
    "#Evaluate on the test set (txt files from directories pos and neg\n",
    "#Assuming X_test is the input data\n",
    "results = model.evaluate(padded_test, bin_list, batch_size=10)\n",
    "\n",
    "#Print the evaluation results\n",
    "print(\"Test Loss:\", results[0])\n",
    "print(\"Test Accuracy:\", results[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[    0     0     0 ...   125  4103   486]\n",
      " [    0     0     0 ...  1977    69   221]\n",
      " [    0     0     0 ...    63    16   350]\n",
      " ...\n",
      " [    0     0     0 ...   125   332   154]\n",
      " [    0     0     0 ...    62   177     5]\n",
      " [    0     0     0 ...    39 50192  1103]]\n"
     ]
    }
   ],
   "source": [
    "##Preprocessing for new model\n",
    "##we concatenate our already padded sequences of the same length along rows\n",
    "combined_sequences = np.concatenate((padded_sequences, padded_test), axis=0)\n",
    "print(combined_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Experiemnt with this, higher seems better but not too high\n",
    "EMBEDDING_DIM = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [],
   "source": [
    "##combine the \"vocabularies\" of each padded reviews\n",
    "combined_index = {**word_index, **word_index_test}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [],
   "source": [
    "##lets define a new model here that combines the two datasets\n",
    "def doc2vec_combined_model():\n",
    "    \n",
    "    ##We have a 2D representation for the tensor, reduce to 1D\n",
    "    ##first dimension will have variable size and second dimension is 2493\n",
    "    input_layer = Input(shape=(X_train.shape[1],))\n",
    "\n",
    "\n",
    "    #Embedding layer\n",
    "    ##word_index is a dictionary, add +1 for the padding token\n",
    "    embedding_layer = Embedding(input_dim=len(combined_index) + 1, output_dim=EMBEDDING_DIM)(input_layer)\n",
    "\n",
    "    #GlobalAveragePooling1D layer\n",
    "    average_pooling = GlobalAveragePooling1D()(embedding_layer)\n",
    "\n",
    "    #Flattening\n",
    "    ##flatten = Flatten()(embedding_layer)\n",
    "\n",
    "    #Dense Layers and Dropout\n",
    "    dense_1 = Dense(64, activation='relu')(average_pooling)\n",
    "    dropout_1 = Dropout(0.2)(dense_1)\n",
    "\n",
    "    dense_2 = Dense(32, activation='relu')(dropout_1)\n",
    "    dropout_2 = Dropout(0.1)(dense_2)\n",
    "\n",
    "    ##Sigmoid as its a binary problem (pos or negative)\n",
    "    output_layer = Dense(1, activation='sigmoid')(dropout_2)\n",
    "\n",
    "\n",
    "    model = Model(inputs=input_layer, outputs=output_layer)\n",
    "\n",
    "    #binary or multi-class compilation\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy', 'mae'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.series.Series'>\n",
      "<class 'pandas.core.series.Series'>\n",
      "(50000,)\n",
      "(25000,)\n",
      "(75000,)\n"
     ]
    }
   ],
   "source": [
    "##split dataset, so this includes moview reviews from directories pos and neg and IMDB_Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "print(type(y))\n",
    "print(type(bin_list))\n",
    "print(y.shape)\n",
    "print(bin_list.shape)\n",
    "\n",
    "combined_label = pd.concat([y, bin_list], ignore_index=True, axis=0)\n",
    "print(combined_label.shape)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(combined_sequences, combined_label, test_size=0.4, random_state=22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "4500/4500 [==============================] - 460s 102ms/step - loss: 0.5197 - accuracy: 0.7129 - mae: 0.3548\n",
      "Epoch 2/2\n",
      "4500/4500 [==============================] - 477s 106ms/step - loss: 0.3238 - accuracy: 0.8650 - mae: 0.1994\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x2b900c01240>"
      ]
     },
     "execution_count": 310,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run IT\n",
    "model = doc2vec_combined_model()\n",
    "model.fit(X_train, y_train, batch_size=10, epochs=2, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3000/3000 [==============================] - 12s 4ms/step - loss: 0.3247 - accuracy: 0.8615 - mae: 0.1899\n",
      "Test Loss: 0.32474032044410706\n",
      "Test Accuracy: 0.861466646194458\n"
     ]
    }
   ],
   "source": [
    "##evaluate new model\n",
    "# Evaluate the model on the test set\n",
    "results = model.evaluate(X_test, y_test, batch_size=10)\n",
    "\n",
    "# Print the evaluation results\n",
    "print(\"Test Loss:\", results[0])\n",
    "print(\"Test Accuracy:\", results[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       how do films like mouse hunt get into theatres...\n",
      "1       some talented actresses are blessed with a dem...\n",
      "2       this has been an extraordinary year for austra...\n",
      "3       according to hollywood movies made in last few...\n",
      "4       my first press screening of 1998 and already i...\n",
      "                              ...                        \n",
      "4741    The man who directed 'The Third Man' also dire...\n",
      "4742    Kevin Spacey is very talented, but unfortunate...\n",
      "4743    Poor Whoopi Goldberg. Imagine her at a friend'...\n",
      "4744    This movie is essentially shot on a hand held ...\n",
      "4745    It has singing. It has drama. It has comedy. I...\n",
      "Name: review, Length: 4746, dtype: object\n",
      "0       0\n",
      "1       0\n",
      "2       1\n",
      "3       1\n",
      "4       0\n",
      "       ..\n",
      "4741    1\n",
      "4742    0\n",
      "4743    0\n",
      "4744    1\n",
      "4745    1\n",
      "Name: label, Length: 4746, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "##preprocessing on third dataset\n",
    "##Read in CSV and drop empty rows\n",
    "df = pd.read_csv('moviereviews.csv')\n",
    "df = df.dropna()\n",
    "df['label'] = df['label'].astype(str)\n",
    "df2 = pd.read_csv('moviereviews2.csv')\n",
    "df2 = df2.dropna()\n",
    "df['label'] = df['label'].astype(str)\n",
    "\n",
    "df_res = pd.concat([df, df2], ignore_index=True)\n",
    "fin_label =df_res['label'].map({'pos': 1, 'neg': 0})\n",
    "fin_reviews = df_res['review']\n",
    "print(fin_reviews)\n",
    "print(fin_label)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [],
   "source": [
    "##same tokenization as before\n",
    "fin_tokenizer = Tokenizer()\n",
    "fin_tokenizer.fit_on_texts(fin_reviews)\n",
    "fin_sequences = tokenizer.texts_to_sequences(fin_reviews)\n",
    "fin_word_index = fin_tokenizer.word_index\n",
    "fin_padded_sequences = pad_sequences(fin_sequences, maxlen=2493)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "475/475 [==============================] - 2s 4ms/step - loss: 0.2432 - accuracy: 0.9069 - mae: 0.1430\n",
      "Test Loss: 0.24318796396255493\n",
      "Test Accuracy: 0.9068689346313477\n"
     ]
    }
   ],
   "source": [
    "fin_res = model.evaluate(fin_padded_sequences, fin_label, batch_size=10)\n",
    "# Print the evaluation results\n",
    "print(\"Test Loss:\", fin_res[0])\n",
    "print(\"Test Accuracy:\", fin_res[1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
