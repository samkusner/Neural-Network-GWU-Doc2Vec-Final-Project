{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Current imports\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "\n",
    "##Tensorflow\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.layers import Embedding, GlobalAveragePooling1D, Dense, Input, Flatten, Dropout\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment\n",
       "0  One of the other reviewers has mentioned that ...  positive\n",
       "1  A wonderful little production. <br /><br />The...  positive\n",
       "2  I thought this was a wonderful way to spend ti...  positive\n",
       "3  Basically there's a family where a little boy ...  negative\n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...  positive"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##Read in CSV\n",
    "df = pd.read_csv('IMDB_Dataset.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    1\n",
      "1    1\n",
      "2    1\n",
      "3    0\n",
      "4    1\n",
      "Name: sentiment, dtype: int64\n",
      "0    One of the other reviewers has mentioned that ...\n",
      "1    A wonderful little production. <br /><br />The...\n",
      "2    I thought this was a wonderful way to spend ti...\n",
      "3    Basically there's a family where a little boy ...\n",
      "4    Petter Mattei's \"Love in the Time of Money\" is...\n",
      "Name: review, dtype: object\n"
     ]
    }
   ],
   "source": [
    "y=df['sentiment'].map({'positive': 1, 'negative': 0})\n",
    "##turn string negative to positive into binary classification\n",
    "print(y.head())\n",
    "\n",
    "reviews=df['review']\n",
    "print(reviews.head())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Preprocessing libraries\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Preprocessing code\n",
    "##remove stop words? ##negligible\n",
    "##lowercase or standardize the format or each word? \n",
    "## case sensitivity might matter in sentiment analysis\n",
    "\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(reviews)\n",
    "sequences = tokenizer.texts_to_sequences(reviews)\n",
    "word_index = tokenizer.word_index\n",
    "padded_sequences = pad_sequences(sequences)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[    0     0     0 ...   125  4103   486]\n",
      " [    0     0     0 ...  1977    69   221]\n",
      " [    0     0     0 ...    63    16   350]\n",
      " ...\n",
      " [    0     0     0 ... 22840     2  6050]\n",
      " [    0     0     0 ...    67   739    42]\n",
      " [    0     0     0 ...   794    11    17]]\n",
      "(50000, 2493)\n"
     ]
    }
   ],
   "source": [
    "print(padded_sequences)\n",
    "print(padded_sequences.shape)\n",
    "##50,000 sequences padded to a length of 2493"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(padded_sequences, y, test_size=0.4, random_state=22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Experiment with this\n",
    "EMBEDDING_DIM = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KerasTensor(type_spec=TensorSpec(shape=(None, 2493), dtype=tf.float32, name='input_29'), name='input_29', description=\"created by layer 'input_29'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 2493), dtype=tf.float32, name='input_30'), name='input_30', description=\"created by layer 'input_30'\")\n"
     ]
    }
   ],
   "source": [
    "##Testing environment\n",
    "print(Input(shape=(padded_sequences.shape[1],)))\n",
    "print(Input(shape=(X_train.shape[1],)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "def doc2vec_model():\n",
    "    \n",
    "    ##We have a 2D representation for the tensor, reduce to 1D which is what we need\n",
    "    ##first dimension will have variable size and second dimension is 2493\n",
    "\n",
    "    input_layer = Input(shape=(X_train.shape[1],))\n",
    "\n",
    "\n",
    "    #Embedding layer\n",
    "    ##word_index is a dictionary, add +1 for the padding token\n",
    "    embedding_layer = Embedding(input_dim=len(word_index) + 1, output_dim=EMBEDDING_DIM)(input_layer)\n",
    "\n",
    "    #GlobalAveragePooling1D layer\n",
    "    ##average_pooling = GlobalAveragePooling1D()(embedding_layer)\n",
    "\n",
    "    #Flattening\n",
    "    flatten = Flatten()(embedding_layer)\n",
    "\n",
    "    # Additional Dense layers with dropout for regularization\n",
    "    dense_1 = Dense(64, activation='relu')(flatten)\n",
    "    dropout_1 = Dropout(0.2)(dense_1)\n",
    "\n",
    "    dense_2 = Dense(32, activation='relu')(dropout_1)\n",
    "    dropout_2 = Dropout(0.1)(dense_2)\n",
    "\n",
    "\n",
    "    #Dense layers\n",
    "    #Optimization such as dropout??\n",
    "\n",
    "    output_layer = Dense(1, activation='sigmoid')(dropout_2)\n",
    "\n",
    "\n",
    "    model = Model(inputs=input_layer, outputs=output_layer)\n",
    "\n",
    "    #binary or multi-class compilation\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "3000/3000 [==============================] - 310s 103ms/step - loss: 0.4701 - accuracy: 0.7380\n",
      "Epoch 2/2\n",
      "3000/3000 [==============================] - 320s 107ms/step - loss: 0.1618 - accuracy: 0.9425\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x2ba1c313040>"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##Train actual model\n",
    "model = doc2vec_model()\n",
    "model.fit(X_train, y_train, batch_size=10, epochs=2, verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000/2000 [==============================] - 12s 6ms/step - loss: 0.2780 - accuracy: 0.8910\n",
      "Test Loss: 0.2779521644115448\n",
      "Test Accuracy: 0.8910499811172485\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the test set\n",
    "results = model.evaluate(X_test, y_test, batch_size=10)\n",
    "\n",
    "# Print the evaluation results\n",
    "print(\"Test Loss:\", results[0])\n",
    "print(\"Test Accuracy:\", results[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def read_txt_files(directory_path):\n",
    "    file_contents = []\n",
    "\n",
    "    # Iterate over each file in the directory\n",
    "    for filename in os.listdir(directory_path):\n",
    "        if filename.endswith(\".txt\"):\n",
    "            file_path = os.path.join(directory_path, filename)\n",
    "            \n",
    "            # Read the content of the file and append it to the list\n",
    "            with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                content = file.read()\n",
    "                file_contents.append(content)\n",
    "\n",
    "    return file_contents\n",
    "\n",
    "pos_list = read_txt_files('pos')\n",
    "neg_list = read_txt_files('neg')\n",
    "\n",
    "# Replace 'your_directory_path' with the path to your directory containing .txt files\n",
    "list_of_strings = pos_list + neg_list\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_list_bin = [1 for i in list(range(12500))]\n",
    "neg_list_bin = [0 for i in list(range(12500))]\n",
    "\n",
    "\n",
    "# Replace 'your_directory_path' with the path to your directory containing .txt files\n",
    "list_of_strings = pos_list + neg_list\n",
    "bin_list = pos_list_bin + neg_list_bin\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_test = Tokenizer()\n",
    "tokenizer_test.fit_on_texts(list_of_strings)\n",
    "sequences_test = tokenizer_test.texts_to_sequences(list_of_strings)\n",
    "word_index_test = tokenizer_test.word_index\n",
    "padded_test = pad_sequences(sequences_test, maxlen=2493)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "bin_list = pd.Series(bin_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "<class 'pandas.core.series.Series'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'pandas.core.series.Series'>\n"
     ]
    }
   ],
   "source": [
    "print(type(padded_sequences))\n",
    "print(type(y))\n",
    "\n",
    "print(type(padded_test))\n",
    "print(type(bin_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2500/2500 [==============================] - 14s 5ms/step - loss: 1.0749 - accuracy: 0.5534\n",
      "Test Loss: 1.0748658180236816\n",
      "Test Accuracy: 0.5534399747848511\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the test set\n",
    "# Assuming X_test is the input data\n",
    "results = model.evaluate(padded_test, bin_list, batch_size=10)\n",
    "\n",
    "# Print the evaluation results\n",
    "print(\"Test Loss:\", results[0])\n",
    "print(\"Test Accuracy:\", results[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   0    0    0 ...  125 4103  486]\n",
      "[[    0     0     0 ...   125  4103   486]\n",
      " [    0     0     0 ...  1977    69   221]\n",
      " [    0     0     0 ...    63    16   350]\n",
      " ...\n",
      " [    0     0     0 ...   125   332   154]\n",
      " [    0     0     0 ...    62   177     5]\n",
      " [    0     0     0 ...    39 50192  1103]]\n"
     ]
    }
   ],
   "source": [
    "##Preprocessing for new model\n",
    "##we concatenate our already padded sequences of the same length along rows\n",
    "combined_sequences = np.concatenate((padded_sequences, padded_test), axis=0)\n",
    "print(combined_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_index = {**word_index, **word_index_test}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "##lets define a new model here that combines the two datasets\n",
    "def doc2vec_combined_model():\n",
    "    \n",
    "    ##We have a 2D representation for the tensor, reduce to 1D which is what we need\n",
    "    ##first dimension will have variable size and second dimension is 2493\n",
    "\n",
    "    input_layer = Input(shape=(X_train.shape[1],))\n",
    "\n",
    "\n",
    "    #Embedding layer\n",
    "    ##word_index is a dictionary, add +1 for the padding token\n",
    "    embedding_layer = Embedding(input_dim=len(combined_index) + 1, output_dim=EMBEDDING_DIM)(input_layer)\n",
    "\n",
    "    #GlobalAveragePooling1D layer\n",
    "    ##average_pooling = GlobalAveragePooling1D()(embedding_layer)\n",
    "\n",
    "    #Flattening\n",
    "    flatten = Flatten()(embedding_layer)\n",
    "\n",
    "    # Additional Dense layers with dropout for regularization\n",
    "    dense_1 = Dense(64, activation='relu')(flatten)\n",
    "    dropout_1 = Dropout(0.2)(dense_1)\n",
    "\n",
    "    dense_2 = Dense(32, activation='relu')(dropout_1)\n",
    "    dropout_2 = Dropout(0.1)(dense_2)\n",
    "\n",
    "\n",
    "    #Dense layers\n",
    "    #Optimization such as dropout??\n",
    "\n",
    "    output_layer = Dense(1, activation='sigmoid')(dropout_2)\n",
    "\n",
    "\n",
    "    model = Model(inputs=input_layer, outputs=output_layer)\n",
    "\n",
    "    #binary or multi-class compilation\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.series.Series'>\n",
      "<class 'pandas.core.series.Series'>\n",
      "(50000,)\n",
      "(25000,)\n",
      "(75000,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "print(type(y))\n",
    "print(type(bin_list))\n",
    "print(y.shape)\n",
    "print(bin_list.shape)\n",
    "\n",
    "combined_label = pd.concat([y, bin_list], ignore_index=True, axis=0)\n",
    "print(combined_label.shape)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(combined_sequences, combined_label, test_size=0.4, random_state=22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "4500/4500 [==============================] - 544s 121ms/step - loss: 0.4572 - accuracy: 0.7645\n",
      "Epoch 2/2\n",
      "4500/4500 [==============================] - 553s 123ms/step - loss: 0.1766 - accuracy: 0.9330\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x2b991dcd9c0>"
      ]
     },
     "execution_count": 257,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##run that model\n",
    "##Train actual model\n",
    "model = doc2vec_combined_model()\n",
    "model.fit(X_train, y_train, batch_size=10, epochs=2, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3000/3000 [==============================] - 17s 6ms/step - loss: 0.3567 - accuracy: 0.8594\n",
      "Test Loss: 0.3566715121269226\n",
      "Test Accuracy: 0.8593999743461609\n"
     ]
    }
   ],
   "source": [
    "##evaluate new model\n",
    "# Evaluate the model on the test set\n",
    "results = model.evaluate(X_test, y_test, batch_size=10)\n",
    "\n",
    "# Print the evaluation results\n",
    "print(\"Test Loss:\", results[0])\n",
    "print(\"Test Accuracy:\", results[1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
